{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85c343a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2,os,torch\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from load_data import load_data,load_data_detection\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import albumentations as albu\n",
    "import torchvision\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37487a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "交界性\t良性  全切组  恶性  正常组\r\n"
     ]
    }
   ],
   "source": [
    "!ls \"./AI基线资料/灰色\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a56ce32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "交界性\t良性  全切组  恶性  正常组\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(564, 1100)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !ls \"./AI基线资料/灰色\"\n",
    "# paths = []\n",
    "# for i in [\"正常组\",\"全切组\"]:\n",
    "#     dirs = os.listdir(\"./AI基线资料/灰色/\" + i)\n",
    "#     dirs = [os.path.join(\"./AI基线资料/灰色/\",i,d) for d in dirs]\n",
    "#     paths = paths + dirs\n",
    "# test = ['2021','2020']\n",
    "# tenum,trnum = [],[]\n",
    "# for p in paths:\n",
    "#     is_test = False\n",
    "#     for t in test:\n",
    "#         if t in p:\n",
    "#             is_test = True\n",
    "#     if is_test:\n",
    "#         tenum.append(p)\n",
    "#     else:\n",
    "#         trnum.append(p)\n",
    "# tenumfig,trnumfig = 0,0\n",
    "# def get_fig_num(paths):\n",
    "#     x = 0\n",
    "#     for p in paths:\n",
    "#         p = os.listdir(p+\"/Images\")\n",
    "#         p = [pp  for pp in p if \".jpg\" in pp]\n",
    "#         x += len(p)\n",
    "#     return x\n",
    "# get_fig_num(tenum),get_fig_num(trnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ca1bf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_detector_one(path='./data',mode='train',verbose=False):\n",
    "    imgs,masks,type_labels,pathology_labels = [],[],[],[]\n",
    "    name = []\n",
    "    \n",
    "    for basepath,dirnames,files in os.walk(path):\n",
    "        jpg = [f for f in files if '.jpg' in f]\n",
    "        if len(jpg) == 0:\n",
    "            continue\n",
    "        for j in jpg:\n",
    "            jpg_path = os.path.join(basepath,j)\n",
    "            img = cv2.imread(jpg_path,0)\n",
    "                \n",
    "            na = basepath.replace(\"/Image\",\"\").split(\"/\")[-1]\n",
    "            imgs.append(img)\n",
    "            type_labels.append(\"exist\")\n",
    "            pathology_labels.append(0)\n",
    "            name.append(na)\n",
    "                \n",
    "    return imgs,type_labels,name\n",
    "def load_data_detector_zero(path='./data',mode='train',verbose=False):\n",
    "    imgs,masks,type_labels,pathology_labels = [],[],[],[]\n",
    "    name = []\n",
    "    \n",
    "    for basepath,dirnames,files in os.walk(path):\n",
    "        jpg = [f for f in files if '.jpg' in f]\n",
    "        if len(jpg) == 0:\n",
    "            continue\n",
    "        for j in jpg:\n",
    "            jpg_path = os.path.join(basepath,j)\n",
    "            img = cv2.imread(jpg_path,0)\n",
    "                \n",
    "            na = basepath.replace(\"/Image\",\"\").split(\"/\")[-1]\n",
    "            imgs.append(img)\n",
    "            type_labels.append(\"no\")\n",
    "            pathology_labels.append(0)\n",
    "            name.append(na)\n",
    "                \n",
    "    return imgs,type_labels,name\n",
    "def load_data_detection(path='./data',mode='train',verbose=False):\n",
    "    types_dict = {'恶性':'exist', '良性':'exist', '交界性':'exist','全切组':'no','正常组':'no'}\n",
    "    types = os.listdir(path)\n",
    "    imgs,masks,type_labels,pathology_labels = [],[],[],[]\n",
    "    name = []\n",
    "    for i in types:\n",
    "        dirnames = os.listdir(os.path.join(path,i))\n",
    "        for d in dirnames:\n",
    "            tmp_path = os.path.join(path,i,d,'Images')\n",
    "            files = os.listdir(tmp_path)\n",
    "            jpg = [f for f in files if '.jpg' in f]\n",
    "            for j in jpg:\n",
    "                jpg_path = os.path.join(tmp_path,j)\n",
    "                img = cv2.imread(jpg_path,0)\n",
    "                \n",
    "                mask = np.zeros_like(img)\n",
    "                if types_dict[i] == 'exist':\n",
    "                    try:\n",
    "                        js = json.load(open(jpg_path.replace('.jpg','.json'),'r'))\n",
    "                    except:\n",
    "                        print(jpg_path)\n",
    "                    for shape in js['shapes']:\n",
    "                        cv2.fillPoly(mask,[np.int32(shape['points'])],color=255)\n",
    "                \n",
    "                if verbose:\n",
    "                    plt.figure(figsize=(10,5))\n",
    "                    plt.subplot(1,2,1);plt.imshow(img,cmap='gray')\n",
    "                    plt.subplot(1,2,2);plt.imshow(mask,cmap='gray')\n",
    "                    plt.title(types_dict[i])\n",
    "                    plt.show()\n",
    "                \n",
    "                imgs.append(img)\n",
    "                masks.append(mask)\n",
    "                type_labels.append(types_dict[i])\n",
    "                pathology_labels.append(0)\n",
    "                name.append(d)\n",
    "    return imgs,masks,type_labels,pathology_labels,name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35f336d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './AI基线资料/灰色/恶性/原发卵巢癌/Images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m imgs,type_labels,name \u001b[38;5;241m=\u001b[39m \u001b[43mload_data_detection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./AI基线资料/灰色/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mload_data_detection\u001b[0;34m(path, mode, verbose)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dirnames:\n\u001b[1;32m     47\u001b[0m     tmp_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path,i,d,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImages\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m     files \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     jpg \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m files \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m f]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m jpg:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './AI基线资料/灰色/恶性/原发卵巢癌/Images'"
     ]
    }
   ],
   "source": [
    "imgs,type_labels,name = load_data_detection(path='./AI基线资料/灰色/',verbose=False)\n",
    "# imgs,type_labels,name = load_data_detection(path='./AI基线资料/灰色/',verbose=False)\n",
    "# imgs,type_labels,name = load_data_detector_one(path='./外部测试图片/外部医院1/灰色/',verbose=False)\n",
    "# imgs1,type_labels1,name1 = load_data_detector_zero(path='./外部测试图片/外部医院1/正常卵巢/',verbose=False)\n",
    "# imgs,type_labels,name = load_data_detector_one(path='./外部测试图片/外部医院2/灰色/',verbose=False)\n",
    "# imgs1,type_labels1,name1 = load_data_detector_zero(path='./外部测试图片/外部医院2/正常/',verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "496de85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = imgs + imgs1\n",
    "type_labels = type_labels + type_labels1\n",
    "name = name + name1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f1fcd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269 178 269 178\n"
     ]
    }
   ],
   "source": [
    "test = ['2021','2020']\n",
    "tr_imgs,tr_masks,tr_type_labels,tr_pathology_labels,tr_name = [],[],[],[],[]\n",
    "te_imgs,te_masks,te_type_labels,te_pathology_labels,te_name = [],[],[],[],[]\n",
    "sta_tr_exist = 0\n",
    "sta_tr_no = 0\n",
    "sta_te_exist = 0\n",
    "sta_te_no = 0\n",
    "\n",
    "for i in range(len(name)):\n",
    "    istest = False\n",
    "    for t in test:\n",
    "        if f'.{t}' in name[i]:\n",
    "            istest = True\n",
    "    if istest or True:\n",
    "        te_imgs.append(imgs[i])\n",
    "        te_type_labels.append(type_labels[i])\n",
    "        te_name.append(name[i])\n",
    "        if type_labels[i] =='exist':\n",
    "            sta_te_exist += 1\n",
    "        else:\n",
    "            sta_te_no += 1\n",
    "        tr_imgs.append(imgs[i])\n",
    "        tr_type_labels.append(type_labels[i])\n",
    "        tr_name.append(name[i])\n",
    "        if type_labels[i] =='exist':\n",
    "            sta_tr_exist += 1\n",
    "        else:\n",
    "            sta_tr_no += 1\n",
    "    else:\n",
    "        tr_imgs.append(imgs[i])\n",
    "        tr_type_labels.append(type_labels[i])\n",
    "        tr_name.append(name[i])\n",
    "        if type_labels[i] =='exist':\n",
    "            sta_tr_exist += 1\n",
    "        else:\n",
    "            sta_tr_no += 1\n",
    "print(sta_tr_exist,sta_tr_no,sta_te_exist,sta_te_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3dc91db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exist': 99, 'no': 96}\n",
      "{'exist': 99, 'no': 96}\n"
     ]
    }
   ],
   "source": [
    "## combine\n",
    "def extract_name(n):\n",
    "    n = n.split('_')\n",
    "    n1,n2 = n[0],n[1]\n",
    "    n1 += '_'\n",
    "    for n22 in n2:\n",
    "        if not (n22.isdigit() or n22 in ['.','C'] ):\n",
    "            n1 += n22\n",
    "    return n1\n",
    "train_case,test_case = {},{}\n",
    "\n",
    "for i,n in enumerate(tr_name):\n",
    "    try:\n",
    "        n1 = extract_name(n)\n",
    "    except:\n",
    "        n1 = n\n",
    "    if not train_case.__contains__(n1):\n",
    "        train_case[n1] = tr_type_labels[i]\n",
    "for i,n in enumerate(te_name):\n",
    "    try:\n",
    "        n1 = extract_name(n)\n",
    "    except:\n",
    "        n1 = n\n",
    "    if not test_case.__contains__(n1):\n",
    "        test_case[n1] = te_type_labels[i]\n",
    "def statistic(case):\n",
    "    sta = {}\n",
    "    for key in case:\n",
    "        gray = case[key]\n",
    "        if not sta.__contains__(gray):\n",
    "            sta[gray] = 1\n",
    "        else:\n",
    "            sta[gray] += 1\n",
    "    print(sta)\n",
    "    sta = {}\n",
    "    for key in case:\n",
    "        gray = case[key]\n",
    "        if not sta.__contains__(gray):\n",
    "            sta[gray] = len(case[key])\n",
    "        else:\n",
    "            sta[gray] += len(case[key])\n",
    "statistic(train_case)\n",
    "statistic(test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6438093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mydataset(Dataset):\n",
    "    def __init__(self,imgs,types_labels,croptumour=False,mode='train'):\n",
    "        self.imgs = imgs\n",
    "        self.tlabels = types_labels\n",
    "        self.type_dict = {'exist':0,'no':1}\n",
    "\n",
    "        self.trans = albu.Compose([\n",
    "            albu.OneOf([\n",
    "                albu.Resize(height=240,width=320),\n",
    "                albu.Compose([\n",
    "                    albu.Resize(height=260,width=340),\n",
    "                    albu.RandomCrop(height=240,width=320)\n",
    "                ])\n",
    "            ],p=1),\n",
    "            albu.ShiftScaleRotate(p=0.5),\n",
    "            albu.VerticalFlip(p=0.1),\n",
    "            albu.HorizontalFlip(p=0.1),\n",
    "        ])\n",
    "        self.testtrans = albu.Compose([\n",
    "            albu.Resize(height=240,width=320),\n",
    "        ])\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        img = self.imgs[index]\n",
    "        \n",
    "        if self.mode == 'test':\n",
    "            trans = self.testtrans(image=img)\n",
    "            img = trans['image']\n",
    "        else:\n",
    "            trans = self.trans(image=img)\n",
    "            img = trans['image']\n",
    "        \n",
    "        tlabel = self.type_dict[self.tlabels[index]]\n",
    "        \n",
    "        img = torch.tensor(img/255).float().unsqueeze(0)\n",
    "        tlabel = torch.tensor(tlabel).long()\n",
    "        return img,tlabel\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91c00bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dataset = mydataset(tr_imgs + te_imgs,tr_type_labels+te_type_labels,mode='train')\n",
    "tr_dataloader = DataLoader(dataset=tr_dataset,batch_size=4,shuffle=True)\n",
    "te_dataset = mydataset(te_imgs,te_type_labels,mode='test')\n",
    "te_dataloader = DataLoader(dataset=te_dataset,batch_size=8,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5bb06d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda:1'\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "# model.features[0]=nn.Conv2d(1,64,3,1,1)\n",
    "# model.classifier[6]=nn.Linear(4096,3,bias=False)\n",
    "model.conv1 = nn.Conv2d(1,64,7,2,3,bias=False)\n",
    "model.fc = nn.Linear(512,2,bias=False)\n",
    "model = torch.load('./weights/detection_gray_2cls_21.pth')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9d5fff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "cri = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(),lr=1e-5,weight_decay=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b31a10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dl):\n",
    "    epoch_loss = 0\n",
    "    for i,(x,t) in enumerate(dl):\n",
    "        x,t = x.to(device),t.to(device)\n",
    "#         x = torch.cat([x,m.unsqueeze(1)*x],dim=1)\n",
    "        p = model(x)\n",
    "        \n",
    "        loss = cri(p,t)\n",
    "        epoch_loss += loss.item() / len(dl)\n",
    "        \n",
    "        opt.zero_grad();loss.backward();opt.step()\n",
    "        \n",
    "        print('\\r{}/{},train_loss:{:.2f}'.format(i,len(dl),loss.item()),end='',flush=True)\n",
    "    return epoch_loss\n",
    "def test(dl):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    preds,ys=[],[]\n",
    "    model.eval()\n",
    "    for i,(x,t) in enumerate(dl):\n",
    "        x,t = x.to(device),t.to(device)\n",
    "#         x = torch.cat([x,m.unsqueeze(1)*x],dim=1)\n",
    "        with torch.no_grad():\n",
    "            p = model(x)\n",
    "            loss = cri(p,t)\n",
    "            epoch_loss += loss.item() / len(dl)\n",
    "            \n",
    "            ys.append(t.detach().cpu())\n",
    "            preds.append(p.detach().cpu())\n",
    "        print('\\r{}/{},test_loss:{:.2f}'.format(i,len(dl),loss.item()),end='',flush=True)\n",
    "        \n",
    "    ys = torch.cat(ys).numpy()\n",
    "    preds = torch.cat(preds)\n",
    "    preds = torch.softmax(preds,dim=1)\n",
    "    argpreds = torch.argmax(preds,dim=1).numpy()\n",
    "    \n",
    "    correct = len(ys[ys==argpreds])\n",
    "    acc = correct / len(ys)\n",
    "    print('-----------------')\n",
    "    print(correct,len(ys))\n",
    "    return epoch_loss,{'acc':acc,'y':ys,'pred':preds.numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e581402a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/100,test_loss:0.6721-----------------\n",
      "703 800\n",
      "-----------------\n",
      "train_loss:0.4512,test_loss:0.3010\n",
      "accuracy:0.8788\n",
      "==> saved model\n",
      "99/100,test_loss:0.0837-----------------\n",
      "687 800\n",
      "-----------------\n",
      "train_loss:0.2757,test_loss:0.3321\n",
      "accuracy:0.8588\n",
      "99/100,test_loss:0.1027-----------------\n",
      "743 800\n",
      "-----------------\n",
      "train_loss:0.2177,test_loss:0.1844\n",
      "accuracy:0.9287\n",
      "==> saved model\n",
      "99/100,test_loss:0.0300-----------------\n",
      "701 800\n",
      "-----------------\n",
      "train_loss:0.2009,test_loss:0.2811\n",
      "accuracy:0.8762\n",
      "99/100,test_loss:0.0333-----------------\n",
      "765 800\n",
      "-----------------\n",
      "train_loss:0.1757,test_loss:0.1440\n",
      "accuracy:0.9563\n",
      "==> saved model\n",
      "99/100,test_loss:0.0507-----------------\n",
      "768 800\n",
      "-----------------\n",
      "train_loss:0.1642,test_loss:0.1722\n",
      "accuracy:0.9600\n",
      "==> saved model\n",
      "99/100,test_loss:0.2100-----------------\n",
      "724 800\n",
      "-----------------\n",
      "train_loss:0.1489,test_loss:0.2002\n",
      "accuracy:0.9050\n",
      "99/100,test_loss:0.0101-----------------\n",
      "772 800\n",
      "-----------------\n",
      "train_loss:0.1434,test_loss:0.0919\n",
      "accuracy:0.9650\n",
      "==> saved model\n",
      "99/100,test_loss:0.0103-----------------\n",
      "772 800\n",
      "-----------------\n",
      "train_loss:0.1350,test_loss:0.0909\n",
      "accuracy:0.9650\n",
      "697/790,train_loss:0.06"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m best_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m200\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     test_loss,metric \u001b[38;5;241m=\u001b[39m test(te_dataloader)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-----------------\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dl)\u001b[0m\n\u001b[1;32m      6\u001b[0m p \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m cri(p,t)\n\u001b[0;32m----> 9\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(dl)\n\u001b[1;32m     11\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad();loss\u001b[38;5;241m.\u001b[39mbackward();opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m,train_loss:\u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i,\u001b[38;5;28mlen\u001b[39m(dl),loss\u001b[38;5;241m.\u001b[39mitem()),end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "for e in range(200):\n",
    "    train_loss = train(tr_dataloader)\n",
    "    test_loss,metric = test(te_dataloader)\n",
    "    print('-----------------')\n",
    "    print('train_loss:{:.4f},test_loss:{:.4f}'.format(train_loss,test_loss))\n",
    "    print('accuracy:{:.4f}'.format(metric['acc'])) \n",
    "    \n",
    "    if best_acc < metric['acc']:\n",
    "        best_acc = metric['acc']\n",
    "        torch.save(model,'./weights/detection_gray_2cls_21.pth')\n",
    "        print('==> saved model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26b6b59",
   "metadata": {},
   "source": [
    "## plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d8b8c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/56,test_loss:6.52-----------------\n",
      "271 447\n"
     ]
    }
   ],
   "source": [
    "test_loss,metric = test(te_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21b0fca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9331, 0.9748)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.9331,0.9748"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "242eb10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------for cate 0--------\n",
      "sensitivity:0.9444,specifity:1.0000\n",
      "auc:1.0000\n",
      "-------for cate 1--------\n",
      "sensitivity:1.0000,specifity:0.9912\n",
      "auc:1.0000\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "def statistic(y,p):\n",
    "    argp = np.argmax(p,axis=1)\n",
    "    uni = np.unique(y)\n",
    "    acc = sklearn.metrics.accuracy_score(y,np.argmax(p,axis=1))\n",
    "    matrix = sklearn.metrics.confusion_matrix(y, argp)\n",
    "    for i in range(matrix.shape[0]):\n",
    "        print(f'-------for cate {i}--------')\n",
    "        TP,FP,TN,FN = 0,0,0,0\n",
    "        for yy,pp in zip(y,argp):\n",
    "            if yy == pp and yy == i:\n",
    "                TP += 1\n",
    "            if yy == pp and yy != i:\n",
    "                TN += 1\n",
    "            if yy != pp and yy == i:\n",
    "                FN += 1\n",
    "            if yy != pp and yy != i:\n",
    "                FP += 1\n",
    "        sensitivity = TP / (TP+FN)\n",
    "        specifity = TP/(TP+FP)\n",
    "        print('sensitivity:{:.4f},specifity:{:.4f}'.format(sensitivity,specifity))\n",
    "        \n",
    "        fpr1, tpr1, thresholds1 = metrics.roc_curve(y,p[:,1], pos_label=1)\n",
    "        roc_auc1 = metrics.auc(fpr1, tpr1)  \n",
    "        print('auc:{:.4f}'.format(roc_auc1))\n",
    "statistic(metric['y'],metric['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "218d603c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score,f1_score,roc_auc_score,recall_score,precision_score\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as pyplot\n",
    "\n",
    "styles=['fivethirtyeight',\n",
    " 'dark_background',\n",
    " 'bmh',\n",
    " 'classic',\n",
    " 'seaborn-dark',\n",
    " 'grayscale',\n",
    " 'seaborn-deep',\n",
    " 'seaborn-muted',\n",
    " 'seaborn-colorblind',\n",
    " 'seaborn-white',\n",
    " 'seaborn-dark-palette',\n",
    " 'ggplot',\n",
    " 'tableau-colorblind10',\n",
    " '_classic_test',\n",
    " 'seaborn-darkgrid',\n",
    " 'seaborn-notebook',\n",
    " 'Solarize_Light2',\n",
    " 'seaborn-paper',\n",
    " 'seaborn-whitegrid',\n",
    " 'seaborn-pastel',\n",
    " 'seaborn-talk',\n",
    " 'seaborn-bright',\n",
    " 'seaborn',\n",
    " 'seaborn-ticks',\n",
    " 'seaborn-poster',\n",
    " 'fast']\n",
    " \n",
    "def rocs_plot(y2, P2, setname=''):\n",
    " \n",
    "    pyplot.figure(figsize=(5, 4), dpi=100)\n",
    "    pyplot.style.use('seaborn-darkgrid')\n",
    "    \n",
    "    palette = pyplot.get_cmap('Set1')\n",
    "    \n",
    "        \n",
    "    for i, p2 in enumerate(P2):\n",
    "        fpr1, tpr1, thresholds1 = metrics.roc_curve(y2,p2, pos_label=1)\n",
    "        roc_auc1 = metrics.auc(fpr1, tpr1)   \n",
    "        pyplot.plot(fpr1, tpr1, color=palette(i), linewidth=1.5, alpha=0.9, label=setname[i][:-10]+\", AUC=%0.3f\" % (roc_auc1))       \n",
    "    pyplot.xlim([0.00, 1.0])\n",
    "    pyplot.ylim([0.00, 1.0])\n",
    "    pyplot.xlabel(\"1-Specificity\",fontsize=12)\n",
    "    pyplot.ylabel(\"Sensitivity\",fontsize=12)\n",
    "    pyplot.title(\"ROC\",fontsize=12)\n",
    "    pyplot.legend(loc=\"lower right\")\n",
    "#     pyplot.savefig('ROC.png',dpi=300)\n",
    "    pyplot.show()\n",
    "rocs_plot(metric['y'],[metric['pred']],setname=['detection'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "095eed9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'preds'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric['pred']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
